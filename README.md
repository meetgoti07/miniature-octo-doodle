# MNIST Digit Recognition - 7 Different Methods ğŸ¤–

[![Hacktoberfest](https://img.shields.io/badge/Hacktoberfest-2025-blueviolet)](https://hacktoberfest.digitalocean.com/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.12+-orange.svg)](https://tensorflow.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-red.svg)](https://pytorch.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-green.svg)](https://scikit-learn.org/)

A comprehensive collection of 7 different machine learning approaches for MNIST digit recognition, ranging from classical algorithms to modern deep learning techniques.

## ğŸ¯ Project Overview

This repository implements digit recognition on the famous MNIST dataset using 7 different methodologies:

1. **Basic Neural Network** - Dense layers with TensorFlow/Keras
2. **Convolutional Neural Network (CNN)** - Custom CNN with data augmentation
3. **PyTorch Implementation** - CNN using PyTorch framework
4. **Support Vector Machine (SVM)** - Multiple kernels with PCA optimization
5. **Random Forest** - Ensemble method with feature engineering
6. **K-Nearest Neighbors (KNN)** - Instance-based learning with distance metrics
7. **Transfer Learning** - Pre-trained models (VGG16, ResNet50, MobileNetV2)

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install -r requirements.txt
```

### Running Individual Models
```bash
# Basic Neural Network
python mnist_basic_nn.py

# Convolutional Neural Network
python mnist_cnn.py

# PyTorch Implementation
python mnist_pytorch.py

# Support Vector Machine
python mnist_svm.py

# Random Forest
python mnist_random_forest.py

# K-Nearest Neighbors
python mnist_knn.py

# Transfer Learning
python mnist_transfer_learning.py
```

## ğŸ“Š Performance Comparison

| Method | Expected Accuracy | Training Time | Pros | Cons |
|--------|------------------|---------------|------|------|
| Basic NN | 98%+ | Fast | Simple, interpretable | Limited feature extraction |
| CNN | 99%+ | Medium | Excellent for images | More complex |
| PyTorch CNN | 99%+ | Medium | Flexible, GPU support | Framework dependency |
| SVM | 95%+ | Medium | Strong theoretical foundation | Memory intensive |
| Random Forest | 94%+ | Fast | No overfitting, interpretable | Less accurate on images |
| KNN | 92%+ | Very Fast | Simple, no training | Slow prediction |
| Transfer Learning | 99.5%+ | Medium | State-of-the-art accuracy | Computationally expensive |

## ğŸ› ï¸ Implementation Details

### 1. Basic Neural Network (`mnist_basic_nn.py`)
- **Architecture**: Dense layers (512â†’256â†’128â†’10)
- **Features**: Dropout regularization, early stopping, learning rate reduction
- **Highlights**: Simple yet effective baseline

### 2. CNN (`mnist_cnn.py`)
- **Architecture**: 3 Conv blocks + Dense layers
- **Features**: Batch normalization, data augmentation, feature map visualization
- **Highlights**: Purpose-built for image classification

### 3. PyTorch CNN (`mnist_pytorch.py`)
- **Architecture**: Custom CNN class with batch normalization
- **Features**: GPU support, learning rate scheduling, detailed metrics
- **Highlights**: Framework flexibility and performance optimization

### 4. SVM (`mnist_svm.py`)
- **Kernels**: Linear, RBF, Polynomial
- **Features**: Grid search optimization, PCA preprocessing, multiple metrics
- **Highlights**: Classical ML approach with solid theoretical foundation

### 5. Random Forest (`mnist_random_forest.py`)
- **Features**: Enhanced pixel features, hyperparameter tuning, feature importance
- **Engineering**: Projections, center of mass, variance calculations
- **Highlights**: Ensemble learning with interpretability

### 6. KNN (`mnist_knn.py`)
- **Metrics**: Euclidean, Manhattan, Minkowski distances
- **Features**: K-value optimization, bias-variance analysis, 2D visualization
- **Highlights**: Instance-based learning with comprehensive analysis

### 7. Transfer Learning (`mnist_transfer_learning.py`)
- **Models**: VGG16, ResNet50, MobileNetV2
- **Features**: Fine-tuning, feature freezing, model comparison
- **Highlights**: Leveraging pre-trained networks for superior performance

## ğŸ“ˆ Visualizations

Each implementation includes comprehensive visualizations:
- **Training curves** (accuracy/loss over epochs)
- **Confusion matrices** for detailed error analysis
- **Feature maps** and learned representations
- **Sample predictions** with confidence scores
- **Model comparisons** and performance metrics

## ğŸ”§ Features

- **Comprehensive Evaluation**: Classification reports, confusion matrices, accuracy metrics
- **Visualization**: Training curves, feature maps, sample predictions
- **Optimization**: Hyperparameter tuning, cross-validation, callbacks
- **Preprocessing**: Data normalization, augmentation, dimensionality reduction
- **Model Persistence**: Save/load trained models for future use

## ğŸ“ Project Structure

```
ğŸ“¦ miniature-octo-doodle
â”œâ”€â”€ ğŸ“„ mnist_basic_nn.py           # Basic Neural Network
â”œâ”€â”€ ğŸ“„ mnist_cnn.py                # Convolutional Neural Network
â”œâ”€â”€ ğŸ“„ mnist_pytorch.py            # PyTorch Implementation
â”œâ”€â”€ ğŸ“„ mnist_svm.py                # Support Vector Machine
â”œâ”€â”€ ğŸ“„ mnist_random_forest.py      # Random Forest Classifier
â”œâ”€â”€ ğŸ“„ mnist_knn.py                # K-Nearest Neighbors
â”œâ”€â”€ ğŸ“„ mnist_transfer_learning.py  # Transfer Learning
â”œâ”€â”€ ğŸ“„ requirements.txt            # Dependencies
â””â”€â”€ ğŸ“„ README.md                   # This file
```

## ğŸ¤ Contributing

This project is part of **Hacktoberfest 2024**! Contributions are welcome:

1. Fork the repository
2. Create a feature branch
3. Implement improvements or new methods
4. Add proper documentation
5. Submit a pull request

### Contribution Ideas
- Add new ML algorithms (Gradient Boosting, Neural Networks variants)
- Improve visualization capabilities
- Add hyperparameter optimization
- Implement ensemble methods
- Add performance benchmarking

## ğŸ“‹ Requirements

```
tensorflow>=2.12.0
numpy>=1.21.0
matplotlib>=3.5.0
seaborn>=0.11.0
scikit-learn>=1.0.0
torch>=1.12.0
torchvision>=0.13.0
opencv-python>=4.5.0
pillow>=8.0.0
pandas>=1.3.0
```

## ğŸ† Results Summary

All implementations achieve competitive results on the MNIST dataset:
- **Best Accuracy**: Transfer Learning (~99.5%)
- **Fastest Training**: Random Forest & KNN
- **Most Interpretable**: Random Forest & SVM
- **Best for Production**: CNN variants

## ğŸ“š Learning Outcomes

This project demonstrates:
- **Classical vs Modern ML**: Comparison of traditional and deep learning approaches
- **Framework Diversity**: TensorFlow, PyTorch, and scikit-learn implementations
- **Optimization Techniques**: Hyperparameter tuning, regularization, data augmentation
- **Evaluation Methods**: Comprehensive metrics and visualization techniques
- **Best Practices**: Code organization, documentation, and reproducibility

## ğŸ“ Educational Value

Perfect for:
- **ML Students**: Understanding different algorithmic approaches
- **Practitioners**: Comparing implementation strategies
- **Researchers**: Baseline implementations for experimentation
- **Developers**: Production-ready code examples

## ğŸ”— References

- [MNIST Database](http://yann.lecun.com/exdb/mnist/)
- [TensorFlow Documentation](https://www.tensorflow.org/)
- [PyTorch Documentation](https://pytorch.org/)
- [scikit-learn Documentation](https://scikit-learn.org/)

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸŒŸ Acknowledgments

- Yann LeCun for the MNIST dataset
- Open source ML community
- Hacktoberfest initiative for promoting open source contributions

---

**â­ If you find this project helpful, please star it and share with others!**

#hacktoberfest #machinelearning #mnist #deeplearning #python #tensorflow #pytorch #scikit-learn
